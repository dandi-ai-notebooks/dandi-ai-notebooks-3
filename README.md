# DANDI AI Notebooks

A system for generating, critiquing, and grading Jupyter notebooks for DANDI datasets using AI models.

Latest results: https://dandi-ai-notebooks.github.io/dandi-ai-notebooks-3/

## Overview

This project automates the process of:
1. Generating Jupyter notebooks that analyze DANDI datasets using AI models
2. Critiquing these notebooks for quality and completeness
3. Grading the notebooks based on specific criteria
4. Exploring and comparing the results through a web interface

## Repository Structure

The main repository contains:

```
dandi-ai-notebooks-3/
├── critiques/          # Notebook critique system
├── gradings/          # Grading system
├── dandisets/         # Submodules for each DANDI dataset
├── templates/         # Generation templates and prompts
├── packages/         # Shared packages
├── scripts/          # Maintenance scripts
└── dandi-ai-notebooks-explorer/  # Web interface
```

### Dandisets as Submodules

Each DANDI dataset has its own repository in the dandi-ai-notebooks GitHub organization. These repositories are included as submodules in the `dandisets/` directory.

To update all submodules and pull latest contents:
```bash
./scripts/update_submodules.sh
```

## Notebook Generation

### Directory Structure

Each dandiset repository contains multiple notebook generation attempts. The directory names follow the format:
```
YYYY-MM-DD-model-name-prompt-type
```

For example: `2025-04-15-claude-3.7-sonnet-prompt-a-3`

### Generation Process

Each notebook generation directory contains:

- `working/` - Workspace for the AI agent
  - `explore/` - Exploratory scripts and plots (for prompts that include exploration)
  - `notebook.py` - Generated notebook source
  - `notebook.ipynb` - Generated executed notebook
  - `minicline.log` - Detailed generation process log
- `[DANISET_ID].ipynb` - Final executed notebook

### Prompts

Available prompts are in `templates/`:
- `prompt-a-*` - Direct notebook generation
- Other prompts - Include data exploration phase

## Maintenance Workflows

After generating new notebooks:
```bash
# Commit changes in submodules
./scripts/commit_submodules.sh

# Update notebooks metadata for GUI
python scripts/create_notebooks_metadata_json.py
```

After running critiques:
```bash
# Create manifest for GUI
python critiques/make_manifest.py
```

After running gradings:
```bash
# Create JSON for GUI
python scripts/create_notebook_gradings_json.py
```

## Web Interface

The explorer web app is automatically deployed to GitHub Pages:
https://dandi-ai-notebooks.github.io/dandi-ai-notebooks-3/

After updating any JSON files (metadata, manifest, gradings), commit and push changes. Note there may be a delay of a few minutes before the GUI reflects changes due to GitHub's API caching.

More details below.

## Grading System

The project includes an automated grading system that evaluates generated notebooks across multiple criteria including data description, loading, visualization, and analysis quality. The complete grading rubric is defined in the [grading system prompt](gradings/templates/gradings_system_prompt.txt).

The grading process:
1. Each notebook is critiqued by an AI model analyzing code cells and outputs
2. The critique feeds into the grading system (using Claude 3.7 Sonnet)
3. Grades and rationales are stored in JSON format under `gradings/dandisets/`
4. Results are incorporated into the web interface for comparison

## Automated Critique Process

Each generated notebook undergoes a thorough critique process:

1. **Cell-by-Cell Analysis** ([prompt](critiques/templates/notebook_critic_cells_system_prompt.txt))
   - Each cell is analyzed individually by Gemini model
   - Analysis includes:
     - Overview of cell's purpose and content
     - Detailed descriptions of any visualizations
     - Identification of potential issues
   - Results stored in `cells_critique.json` and `cells_critique.txt`

2. **Summary Critique** ([prompt](critiques/templates/notebook_critic_summary_system_prompt.txt))
   - Generated by Claude model using cell critiques as input
   - Evaluates the notebook as a whole focusing on:
     - How well it introduces and explains the dataset
     - Quality of data loading and visualization
     - Effectiveness in guiding further analysis
     - Overall issues and their severity
   - Stored in `summary_critique.txt`

3. **Critique Storage**
   - All critiques stored under `critiques/dandisets/[DANDISET_ID]/[GENERATION_DIR]/`
   - Integrated into the web interface for easy review

## Prompt Types

### prompt-a Series
Direct notebook generation without preliminary exploration phase. Each incremental version (prompt-a-1, prompt-a-2, etc.) represents an iteration of improvements to the generation process, with higher numbers indicating more recent versions.

For example: [prompt-a-5.txt](templates/prompt-a-5.txt)

### prompt-b Series
Enhanced notebook generation that includes a preliminary data exploration phase before creating the notebook. This allows the AI to better understand the dataset structure and contents before generating analysis code. Like the 'a' series, version numbers indicate iterations of improvements.

For example: [prompt-b-5.txt](templates/prompt-b-5.txt)

### prompt-d Series
Similar to the 'b' series but includes an additional notebook critique phase. After initial generation, the notebook undergoes a critique operation, and based on the feedback received, the AI may modify the notebook to create an improved final version.

For example: [prompt-d-5.txt](templates/prompt-d-5.txt)

## Development

### Prerequisites
- Python 3.x
- Node.js v20+
- Set environment variable `OPENAI_API_KEY` for OpenAI API access

### Setup
Clone the repository

Initialize and update submodules (see above)

Install minicline:

```bash
pip install minicline
```

Install get-nwbfile-info

```bash
# get-nwbfile-info is a submodule
cd packages/get-nwbfile-info
pip install -e .
```

To run the web interface locally, navigate to `dandi-ai-notebooks-explorer/` and run:

```bash
npm install
npm run dev
```

### Core Components

#### Notebook Generation (`generate_notebook.py`)
- Creates notebook directories with template files
- Configures AI model and prompt settings
- Manages the notebook generation process

#### Critiques System (`critiques/`)
- Evaluates generated notebooks using AI models
- Produces detailed cell-by-cell critiques
- Generates summary critiques
- Uses different models for cells (Gemini) and summaries (Claude)

#### Grading System (`gradings/`)
- Assigns numerical grades based on specific criteria
- Processes critique outputs for structured evaluation
- Stores results in both JSON and text formats

#### Web Explorer (`dandi-ai-notebooks-explorer/`)

The web explorer provides an interactive interface to browse, compare, and analyze notebooks generated for different DANDI datasets. Key features include:

* Interactive table displaying all generated notebooks with:
  * Links to view notebooks, critiques, and gradings
  * Filtering by dandiset ID
  * Sorting by various metrics (subfolder, model, prompt type, cost, etc.)
  * Estimated cost calculations for each notebook generation
* Column information:
  * Notebook - Direct link to the generated notebook
  * Subfolder - Generation attempt identifier
  * Dandiset - Dataset ID
  * Model - AI model used for generation
  * Prompt - Link to prompt template used
  * Est. Cost - Calculated token cost for generation
  * Critiques - Links to cell-by-cell and summary critiques
  * Grade - Total grade with detailed breakdown

### Development

```bash
cd dandi-ai-notebooks-explorer
npm install
npm run dev
```

The interface is automatically deployed to GitHub Pages when changes are pushed to the main branch:
https://dandi-ai-notebooks.github.io/dandi-ai-notebooks-3/

Updates to metadata files (notebooks, critiques, rankings, gradings) require a commit and push. Changes may take a few minutes to appear due to GitHub's API caching.
